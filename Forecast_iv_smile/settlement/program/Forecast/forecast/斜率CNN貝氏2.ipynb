{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from function import Slope, minSSE_recovery\n",
    "\n",
    "s_c = '結算價'\n",
    "top_path = './../../../'\n",
    "Data_path = top_path + 'InterpData/'\n",
    "expiry = 'NearbyMonth'\n",
    "\n",
    "IV_type = 'callIV'\n",
    "K_Range = [300, 500]\n",
    "K_Range_file = '{}_{}.csv'.format(K_Range[0], K_Range[1])\n",
    "K_Range_dir = '{}_{}'.format(K_Range[0], K_Range[1])\n",
    "IV_path = '{}/{}/{}/{}'.format(Data_path, expiry, IV_type, K_Range_file)\n",
    "\n",
    "Dir_tree = [top_path, 'ForecastData', expiry, IV_type, K_Range_dir]\n",
    "current_path = Dir_tree[0]\n",
    "for i in range(1, len(Dir_tree), 1):\n",
    "    if Dir_tree[i] not in os.listdir(current_path):\n",
    "        os.mkdir(current_path + Dir_tree[i])\n",
    "    current_path = current_path + Dir_tree[i] + '/'\n",
    "\n",
    "IV_data = pd.read_csv(IV_path, encoding='Big5', index_col=False)\n",
    "IV_matrix = np.array(IV_data)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "OC = np.array(IV_data['期貨開盤價'] - IV_data['期貨收盤價'])\n",
    "HL = np.array(IV_data['期貨最高價'] - IV_data['期貨最低價'])\n",
    "expirty_days = np.array(IV_data['到期天數'])\n",
    "#OC_HL_K_E = np.vstack((OC, HL, Kmin_reduce_F, Kmax_reduce_F, expirty_days)).T\n",
    "OCHL = np.vstack((OC, HL)).T\n",
    "\n",
    "\n",
    "IV_matrix = np.array(IV_data)\n",
    "K_num = len(np.where(IV_matrix[0, 0] == IV_matrix)[0])\n",
    "K = np.array(IV_data['履約價'])\n",
    "K = np.reshape(K, (-1, K_num))\n",
    "IV= np.array(IV_data['隱含波動率({})'.format(s_c)])\n",
    "IV = np.reshape(IV, (-1, K_num))\n",
    "K_IVslope = Slope(X=K, Y=IV, axis=1)\n",
    "E = IV_matrix[range(0, len(IV_matrix), K_num), IV_data.columns.get_loc('到期天數')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsia\n",
    "def _markov_transition_field(X_binned, X_mtm, n_timestamps):\n",
    "    X_mtf = np.zeros((n_timestamps, n_timestamps))\n",
    "    \n",
    "    # We loop through each timestamp twice to build a N x N matrix:\n",
    "    for i in range(n_timestamps):\n",
    "        for j in range(n_timestamps):\n",
    "            # We align each probability along the temporal order: MTF(i,j) \n",
    "            # denotes the transition probability of the bin 'i' to the bin \n",
    "            # 'j':\n",
    "            X_mtf[i, j] = X_mtm[X_binned[i], X_binned[j]]\n",
    "            \n",
    "    return X_mtf\n",
    "\n",
    "\n",
    "one_day_expiry_idx = np.arange(len(E))[np.equal(E, 1)]\n",
    "if one_day_expiry_idx[-1] == len(E)-1:\n",
    "    one_day_expiry_idx = one_day_expiry_idx[:-1]\n",
    "most_days_expiry_idx = one_day_expiry_idx + 1\n",
    "most_days_expiry_idx = np.hstack((0, most_days_expiry_idx))\n",
    "\n",
    "most_days_expiry_idx0 = most_days_expiry_idx[:-1]\n",
    "most_days_expiry_idx1 = most_days_expiry_idx[1:]\n",
    "trade_days_in_month = most_days_expiry_idx1 - most_days_expiry_idx0  \n",
    "most_days_expiry = E[most_days_expiry_idx]\n",
    "\n",
    "contract_appear_days1 = [list(range(1, trade_day_in_month+1, 1)) \\\n",
    "                for trade_day_in_month in trade_days_in_month]\n",
    "contract_appear_days1 = [contract_appear_day1 for subcontract_appear_day1 in contract_appear_days1\\\n",
    "                         for contract_appear_day1 in subcontract_appear_day1]\n",
    "contract_appear_days1 = np.array(contract_appear_days1)\n",
    "contract_appear_days2 = np.arange(1, len(E) - most_days_expiry_idx[-1] +1, 1)\n",
    "contract_appear_days = np.hstack((contract_appear_days1, contract_appear_days2))\n",
    "#contract_appear_days 為該契約(特定交易日期、到期日期，履約價不限)上市的交易日數，例如第一天上市到期天數35天，\n",
    "#則值為1，第二天則到期日為34天值為2，一直到k(因為是交易日數，所以數字不一定)。然後又到下個月的契約，值又從1開\n",
    "#始\n",
    "\n",
    "\n",
    "from function import TimeSeriesData\n",
    "seq_length1 = 7\n",
    "seq_length2 = seq_length1 + 1\n",
    "min_contract_day = 2\n",
    "magnification_slope = 10000\n",
    "Q = 4\n",
    "\n",
    "#變數有考慮到期日\n",
    "#K_IVslope_E = np.hstack((K_IVslope*magnification_slope, np.reshape(E, (-1, 1))))\n",
    "\n",
    "#Inputs_slope, Ouputs = TimeSeriesData(K_IVslope_E, seq_length=seq_length1, \\\n",
    "                                      #drop_out_columns=[len(K_IVslope_E[0])-1])\n",
    "\n",
    "#變數沒考慮到期日\n",
    "K_IVslope_E = K_IVslope * magnification_slope\n",
    "Inputs_slope, Ouputs = TimeSeriesData(K_IVslope_E, seq_length=seq_length1, drop_out_columns=[])\n",
    "\n",
    "\n",
    "\n",
    "Inputs_OC  = OC[range(0, len(IV_matrix), K_num)]\n",
    "Inputs_OC  = np.array([Inputs_OC[i:i+seq_length2] \\\n",
    "                              for i in range(len(Inputs_OC)-seq_length2+1)])\n",
    "Inputs_OC  = Inputs_OC.astype(float)\n",
    "Inputs_HL  = HL[range(0, len(IV_matrix), K_num)]\n",
    "Inputs_HL  = np.array([Inputs_HL[i:i+seq_length2] \\\n",
    "                              for i in range(len(Inputs_HL)-seq_length2+1)])\n",
    "Inputs_HL  = Inputs_HL.astype(float)\n",
    "if seq_length1  > seq_length2-1:\n",
    "    Inputs_OC = Inputs_OC[seq_length1-seq_length2+1:]\n",
    "    Inputs_HL = Inputs_HL[seq_length1-seq_length2+1:]\n",
    "    contract_appear_days = contract_appear_days[seq_length1-seq_length2+1:]\n",
    "    IV_matrix_forecast = IV_matrix[seq_length1*K_num:]\n",
    "if seq_length1 <= seq_length2-1:\n",
    "    Inputs_slope = Inputs_slope[seq_length2-1-seq_length1:]\n",
    "    Ouputs = Ouputs[seq_length2-1-seq_length1:]\n",
    "    contract_appear_days = contract_appear_days[seq_length2-1:]\n",
    "    IV_matrix_forecast = IV_matrix[(seq_length2-1)*K_num:]\n",
    "\n",
    "\n",
    "match_cond = np.greater_equal(contract_appear_days, min_contract_day)\n",
    "Inputs_OC = Inputs_OC[match_cond]\n",
    "Inputs_HL = Inputs_HL[match_cond]\n",
    "Inputs_slope = Inputs_slope[match_cond]\n",
    "contract_appear_days_filter = contract_appear_days[match_cond]\n",
    "contract_appear_days_filter_Knum = contract_appear_days_filter.repeat(K_num)\n",
    "match_cond_Knum = match_cond.repeat(K_num)\n",
    "IV_matrix_forecast = IV_matrix_forecast[match_cond_Knum]\n",
    "IV_matrix_forecast= np.hstack((IV_matrix_forecast, np.reshape(contract_appear_days_filter_Knum, (-1, 1))))\n",
    "Ouputs = Ouputs[match_cond]\n",
    "\n",
    "\n",
    "GGM_OC = np.zeros((len(Inputs_OC), seq_length2, seq_length2, 3))\n",
    "for i in range(len(Inputs_OC)):\n",
    "    X = Inputs_OC[i]\n",
    "    X_hat = ((X-X.max()) + (X-X.min())) / (X.max() - X.min())\n",
    "    X_hat = np.reshape(X_hat,(1,-1))\n",
    "    y = np.sqrt(np.ones((1,X_hat.size)) - X_hat**2)\n",
    "    Gcos = X_hat.T @ X_hat - (y.T @ y)\n",
    "\n",
    "    y = np.sqrt(np.ones((1,X_hat.size)) - X_hat**2)\n",
    "    Gsin = y.T @ X_hat + (y.T @ X_hat).T\n",
    "\n",
    "    X_ = pd.DataFrame(X)\n",
    "    X_binned, bin_edges = tsia.markov.discretize(X_, Q)\n",
    "\n",
    "    X_mtm = tsia.markov.markov_transition_matrix(X_binned)\n",
    "\n",
    "    X_mtm = tsia.markov.markov_transition_probabilities(X_mtm)\n",
    "\n",
    "    X_mtf = _markov_transition_field(X_binned, X_mtm, seq_length2)\n",
    "    \n",
    "    GGM_OC[i, :,:,0] = Gcos \n",
    "    GGM_OC[i, :,:,1] = Gsin \n",
    "    GGM_OC[i, :,:,2] = X_mtf \n",
    "\n",
    "GGM_HL = np.zeros((len(Inputs_HL), seq_length2, seq_length2, 3))\n",
    "for i in range(len(Inputs_HL)):\n",
    "    X = Inputs_HL[i]\n",
    "    X_hat = ((X-X.max()) + (X-X.min())) / (X.max() - X.min())\n",
    "    X_hat = np.reshape(X_hat,(1,-1))\n",
    "    y = np.sqrt(np.ones((1,X_hat.size)) - X_hat**2)\n",
    "    Gcos = X_hat.T @ X_hat - (y.T @ y)\n",
    "\n",
    "    y = np.sqrt(np.ones((1,X_hat.size)) - X_hat**2)\n",
    "    Gsin = y.T @ X_hat + (y.T @ X_hat).T\n",
    "\n",
    "    X_ = pd.DataFrame(X)\n",
    "    X_binned, bin_edges = tsia.markov.discretize(X_, Q)\n",
    "\n",
    "    X_mtm = tsia.markov.markov_transition_matrix(X_binned)\n",
    "\n",
    "    X_mtm = tsia.markov.markov_transition_probabilities(X_mtm)\n",
    "\n",
    "    X_mtf = _markov_transition_field(X_binned, X_mtm, seq_length2)\n",
    "    \n",
    "    GGM_HL[i, :,:,0] = Gcos \n",
    "    GGM_HL[i, :,:,1] = Gsin \n",
    "    GGM_HL[i, :,:,2] = X_mtf \n",
    "\n",
    "train_size = int(len(Inputs_slope)*0.8)\n",
    "X_train = Inputs_slope[:train_size]\n",
    "OC_train = GGM_OC[:train_size ]\n",
    "HL_train = GGM_HL[:train_size ]\n",
    "y_train = Ouputs[:train_size]\n",
    "\n",
    "\n",
    "\n",
    "X_test = Inputs_slope[train_size:]\n",
    "y_test = Ouputs[train_size:]\n",
    "IV_matrix_test = IV_matrix_forecast[K_num*train_size:]\n",
    "OC_test = GGM_OC[train_size:]\n",
    "HL_test = GGM_HL[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 50ms/step - loss: 7.1488 - mse: 4.9181 - val_loss: 0.7768 - val_mse: 0.6310\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 3.2702 - mse: 3.1641 - val_loss: 0.6512 - val_mse: 0.6082\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 30ms/step - loss: 3.4057 - mse: 3.3164 - val_loss: 0.6858 - val_mse: 0.6380\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - loss: 3.0280 - mse: 2.9858 - val_loss: 0.6771 - val_mse: 0.6464\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 86ms/step - loss: 3.8404 - mse: 3.5561 - val_loss: 0.7702 - val_mse: 0.6631\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - loss: 2.5359 - mse: 2.4439 - val_loss: 0.6713 - val_mse: 0.6113\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - loss: 4.6598 - mse: 3.4937 - val_loss: 0.7562 - val_mse: 0.6093\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - loss: 2.8745 - mse: 2.7609 - val_loss: 0.6636 - val_mse: 0.6031\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 91ms/step - loss: 4.0547 - mse: 4.0449 - val_loss: 0.6428 - val_mse: 0.6330\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - loss: 3.3881 - mse: 3.3784 - val_loss: 0.6365 - val_mse: 0.6269\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 79ms/step - loss: 4.7762 - mse: 3.8472 - val_loss: 1.2674 - val_mse: 0.7140\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - loss: 4.0225 - mse: 3.5203 - val_loss: 1.1079 - val_mse: 0.7219\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - loss: 3.7953 - mse: 3.4501 - val_loss: 0.8645 - val_mse: 0.7602\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 70ms/step - loss: 2.4953 - mse: 2.4073 - val_loss: 0.7251 - val_mse: 0.6709\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - loss: 6.1567 - mse: 3.8103 - val_loss: 1.7049 - val_mse: 0.6418\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - loss: 3.8432 - mse: 2.9333 - val_loss: 1.2300 - val_mse: 0.6255\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - loss: 5.6425 - mse: 4.0409 - val_loss: 1.3739 - val_mse: 0.6630\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 3.2716 - mse: 2.6554 - val_loss: 1.0745 - val_mse: 0.6429\n",
      "Restoring model weights from the end of the best epoch: 2.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Epoch 1/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 3.5403 - mse: 3.4523 - val_loss: 0.7245 - val_mse: 0.6457\n",
      "Epoch 2/2\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 42ms/step - loss: 2.0962 - mse: 2.0194 - val_loss: 0.7432 - val_mse: 0.6714\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "最佳參數: [4, 5, 36, 101, 6, 4, 80, 20, 521, 325, 3.807158379249398e-05, 0.005089035297840717, 0.0008111253665497065, 2.023778954048509e-05]\n",
      "最小損失: 0.0022552991229109025\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM, concatenate, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Integer, Real\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.acquisition import gaussian_ei\n",
    "n_calls=10\n",
    "\n",
    "lstm_neurons_num = 400\n",
    "lstm_layers_num = 1\n",
    "dense1_neurons_num = 200\n",
    "dense2_neurons_num=  150\n",
    "dense3_neurons_num = 360\n",
    "\n",
    "def set_CNN_LSTM(lstm_structure, cnn1_structure, cnn2_structure, Megred_struture):\n",
    "    lstm_neurons_num = lstm_structure['lstm_neurons_num']\n",
    "    lstm_activations = lstm_structure['lstm_activations']\n",
    "    lstm_recurrent_activations = lstm_structure['lstm_recurrent_activations']\n",
    "    dense1_neurons_num = lstm_structure['dense_neurons_num']\n",
    "    dense1_activations = lstm_structure['dense_activations']\n",
    "    kernel_regularizer1= lstm_structure['kernel_regularizer']\n",
    "\n",
    "    cnn1_kernel_size = cnn1_structure['cnn_kernel_size']\n",
    "    cnn1_filters = cnn1_structure['cnn_filters']\n",
    "    cnn1_strides = cnn1_structure['cnn_stride']\n",
    "    cnn1_padding = cnn1_structure['cnn_padding']\n",
    "    cnn1_activations = cnn1_structure['cnn_activations']\n",
    "    dense2_neurons_num = cnn1_structure['dense_neurons_num']\n",
    "    dense2_activations = cnn1_structure['dense_activations']\n",
    "    kernel_regularizer2= cnn1_structure['kernel_regularizer']\n",
    "\n",
    "    cnn2_kernel_size = cnn2_structure['cnn_kernel_size']\n",
    "    cnn2_filters = cnn2_structure['cnn_filters']\n",
    "    cnn2_strides = cnn2_structure['cnn_stride']\n",
    "    cnn2_padding = cnn2_structure['cnn_padding']\n",
    "    cnn2_activations = cnn2_structure['cnn_activations']\n",
    "    dense3_neurons_num = cnn2_structure['dense_neurons_num']\n",
    "    dense3_activations = cnn2_structure['dense_activations']\n",
    "    kernel_regularizer3= cnn2_structure['kernel_regularizer']\n",
    "\n",
    "    \n",
    "    Merged_neurons_num = Megred_struture['neurons_num']\n",
    "    Megred_activations = Megred_struture['activations']\n",
    "    Merged_kernel_regularizer = Megred_struture['kernel_regularizer']\n",
    "\n",
    "    input_lstm = Input(shape=lstm_structure['input_shape'])\n",
    "    hidden_1 = input_lstm\n",
    "\n",
    "    for i in range(0, len(lstm_neurons_num), 1):\n",
    "        if i == len(lstm_neurons_num) - 1:\n",
    "            return_sequences=False\n",
    "        else:\n",
    "            return_sequences=True \n",
    "        hidden_1 = LSTM(lstm_neurons_num[i], return_sequences=return_sequences,\\\n",
    "        activation=lstm_activations[i], kernel_regularizer=kernel_regularizer1[i],\n",
    "        recurrent_activation= lstm_recurrent_activations[i])(hidden_1)\n",
    "    output_1 = hidden_1\n",
    "    for i in range(0, len(dense1_neurons_num), 1):\n",
    "        output_1 = Dense(dense1_neurons_num[i], \\\n",
    "            kernel_regularizer=kernel_regularizer1[len(lstm_neurons_num)-1+i], \\\n",
    "            activation=dense1_activations[i])(output_1)\n",
    "        \n",
    "    input_cnn1 = Input(shape=cnn1_structure['input_shape'])\n",
    "    hidden_2 = input_cnn1\n",
    "\n",
    "    for i in range(0, len(cnn1_kernel_size), 1):\n",
    "        hidden_2 =  Conv2D(filters=cnn1_filters[i],\n",
    "                           kernel_size=cnn1_kernel_size[i],\n",
    "                           strides=cnn1_strides[i],\n",
    "                           padding=cnn1_padding[i],\n",
    "                           activation=cnn1_activations[i],\n",
    "                           kernel_regularizer=kernel_regularizer2[i]\n",
    "                        )(hidden_2)\n",
    "        #hidden_2  = MaxPooling2D(pool_size=(2, 2))(hidden_2 )\n",
    "    output_2 = Flatten()(hidden_2)\n",
    "    for i in range(0, len(dense2_neurons_num), 2):\n",
    "        output_2 = Dense(dense2_neurons_num[i], \\\n",
    "            kernel_regularizer=kernel_regularizer2[len(cnn1_kernel_size)-1+i], \\\n",
    "            activation=dense2_activations[i])(output_2)\n",
    "    input_cnn2 = Input(shape=cnn2_structure['input_shape'])\n",
    "    hidden_3 = input_cnn2\n",
    "    \n",
    "    for i in range(0, len(cnn2_kernel_size), 1):\n",
    "        hidden_3 =  Conv2D(filters=cnn2_filters[i],\n",
    "                           kernel_size=cnn2_kernel_size[i],\n",
    "                           strides=cnn2_strides[i],\n",
    "                           padding=cnn2_padding[i],\n",
    "                           activation=cnn2_activations[i],\n",
    "                           kernel_regularizer=kernel_regularizer3[i]\n",
    "                        )(hidden_3)\n",
    "        #hidden_3  = MaxPooling2D(pool_size=(2, 2))(hidden_3 )\n",
    "    output_3 = Flatten()(hidden_3)\n",
    "    for i in range(0, len(dense3_neurons_num), 2):\n",
    "        output_3 = Dense(dense3_neurons_num[i], \\\n",
    "            kernel_regularizer=kernel_regularizer3[len(cnn2_kernel_size)-1+i], \\\n",
    "            activation=dense3_activations[i])(output_3)\n",
    "    merged = concatenate([output_1, output_2, output_3], axis=-1)\n",
    "    dense = merged\n",
    "    for i in range(len(Merged_neurons_num)):\n",
    "        dense = Dense(Merged_neurons_num[i], Megred_activations[i], Merged_kernel_regularizer[i])(dense)\n",
    "\n",
    "    output = Dense(y_train.shape[1])(dense)\n",
    "    model = Model(inputs=[input_lstm, input_cnn1, input_cnn2], outputs=output)\n",
    "    return model\n",
    "\n",
    "search_space = [\n",
    "    Integer(3, 9, name='cnn1_kernel_size1'),\n",
    "    Integer(3, 9, name='cnn1_kernel_size2'),\n",
    "    Integer(16, 128, name='cnn1_filter1'),  \n",
    "    Integer(16, 128, name='cnn1_filter2'),  \n",
    "    Integer(3, 9, name='cnn2_kernel_size1'),\n",
    "    Integer(3, 9, name='cnn2_kernel_size2'), \n",
    "    Integer(16, 128, name='cnn2_filter1'),  \n",
    "    Integer(16, 128, name='cnn2_filter2'),  \n",
    "    Integer(100, 600, name='merged_neurons_num1'),\n",
    "    Integer(100, 600, name='merged_neurons_num2'),\n",
    "    Real(1e-6, 1e-2, prior='log-uniform', name='lambda1'),\n",
    "    Real(1e-6, 1e-2, prior='log-uniform', name='lambda2'),\n",
    "    Real(1e-6, 1e-2, prior='log-uniform', name='lambda3'),\n",
    "    Real(1e-6, 1e-2, prior='log-uniform', name='lambda4')\n",
    "]\n",
    "\n",
    "def train_and_evaluate_net(cnn1_kernel_size1, cnn1_kernel_size2, cnn1_filter1, cnn1_filter2,\\\n",
    "                           cnn2_kernel_size1, cnn2_kernel_size2, cnn2_filter1, cnn2_filter2,\n",
    "                           merged_neurons_num1, merged_neurons_num2, \\\n",
    "                            lambda1, lambda2, lambda3, lambda4):\n",
    "    batch_size = 32\n",
    "    epochs = 2\n",
    "    \n",
    "    Slope_lstm_neurons_num = [int(lstm_neurons_num)]*lstm_layers_num\n",
    "    Slope_lstm_activations = ['tanh']*len(Slope_lstm_neurons_num)\n",
    "    Slope_lstm_recurrent_activations = ['sigmoid']*len(Slope_lstm_neurons_num)\n",
    "    Slope_dense_neurons_num = [dense1_neurons_num]\n",
    "    Slope_dense_activations = ['relu']\n",
    "    Slope_kernel_regularizer=[l2(lambda1)]*(len(Slope_lstm_neurons_num) + len(Slope_dense_neurons_num))\n",
    "\n",
    "    OC_cnn_kernel_size = [(cnn1_kernel_size1, cnn1_kernel_size1),\n",
    "                          (cnn1_kernel_size2, cnn1_kernel_size2)]\n",
    "    cnn1_layers_num = len(OC_cnn_kernel_size)\n",
    "    OC_cnn_filters = [cnn1_filter1, cnn1_filter2] \n",
    "    OC_cnn_strides = [(1,1)] * cnn1_layers_num\n",
    "    OC_cnn_padding = ['same'] * cnn1_layers_num \n",
    "    OC_cnn_activations = ['relu']*len(OC_cnn_kernel_size)\n",
    "    OC_dense_neurons_num = [dense2_neurons_num]\n",
    "    OC_dense_activations = ['relu']\n",
    "    OC_kernel_regularizer=[l2(lambda2)]*(len(OC_cnn_kernel_size) + len(OC_dense_neurons_num))\n",
    "\n",
    "    HL_cnn_kernel_size = [(cnn2_kernel_size1, cnn2_kernel_size1),\n",
    "                          (cnn2_kernel_size2, cnn2_kernel_size2)]\n",
    "    cnn2_layers_num = len(HL_cnn_kernel_size)\n",
    "    HL_cnn_filters = [cnn2_filter1, cnn2_filter2] \n",
    "    HL_cnn_strides = [(1,1)] * cnn2_layers_num\n",
    "    HL_cnn_padding = ['same'] * cnn2_layers_num \n",
    "    HL_cnn_activations = ['relu']*len(HL_cnn_kernel_size)\n",
    "    HL_dense_neurons_num = [dense3_neurons_num]\n",
    "    HL_dense_activations = ['relu']\n",
    "    HL_kernel_regularizer=[l2(lambda3)]*(len(HL_cnn_kernel_size) + len(HL_dense_neurons_num))\n",
    "\n",
    "    \n",
    "\n",
    "    Merged_neurons_num = [merged_neurons_num1, merged_neurons_num2]\n",
    "    Megred_activations = ['relu']*len(Merged_neurons_num)\n",
    "    Merged_kernel_regularizer=[l2(lambda4)]*(len(Merged_neurons_num))\n",
    "    lstm_Slope_structure = {\n",
    "        'input_shape': X_train.shape[1:],\n",
    "        'lstm_neurons_num': Slope_lstm_neurons_num,\n",
    "        'lstm_activations': Slope_lstm_activations,\n",
    "        'lstm_recurrent_activations':Slope_lstm_recurrent_activations,\n",
    "        'dense_neurons_num':Slope_dense_neurons_num,\n",
    "        'dense_activations':Slope_dense_activations,\n",
    "        'kernel_regularizer':Slope_kernel_regularizer\n",
    "        }\n",
    "    \n",
    "    OC_cnn_structure = {\n",
    "    'input_shape': OC_train.shape[1:],\n",
    "    'cnn_kernel_size': OC_cnn_kernel_size,\n",
    "    'cnn_filters': OC_cnn_filters,\n",
    "    'cnn_stride': OC_cnn_strides,\n",
    "    'cnn_padding': OC_cnn_padding,\n",
    "    'cnn_activations': OC_cnn_activations,\n",
    "    'dense_neurons_num': OC_dense_neurons_num,\n",
    "    'dense_activations': OC_dense_activations,\n",
    "    'kernel_regularizer': OC_kernel_regularizer\n",
    "    }\n",
    "\n",
    "    HL_cnn_structure = {\n",
    "    'input_shape': HL_train.shape[1:],\n",
    "    'cnn_kernel_size': HL_cnn_kernel_size,\n",
    "    'cnn_filters': HL_cnn_filters,\n",
    "    'cnn_stride': HL_cnn_strides,\n",
    "    'cnn_padding': HL_cnn_padding,\n",
    "    'cnn_activations': HL_cnn_activations,\n",
    "    'dense_neurons_num': HL_dense_neurons_num,\n",
    "    'dense_activations': HL_dense_activations,\n",
    "    'kernel_regularizer': HL_kernel_regularizer\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    Merged_structure = {\n",
    "        'neurons_num': Merged_neurons_num,\n",
    "        'activations': Megred_activations,\n",
    "        'kernel_regularizer': Merged_kernel_regularizer\n",
    "        }\n",
    "    alltrain = 1\n",
    "    allMSE = np.zeros(alltrain) \n",
    "    for j in range(alltrain):\n",
    "        #設定模型結構\n",
    "        model = set_CNN_LSTM(lstm_Slope_structure, OC_cnn_structure, HL_cnn_structure, Merged_structure)\n",
    "        model.compile(loss='mse', optimizer = 'adam', metrics=['mse'])  \n",
    "\n",
    "        # 訓練模型\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=10, \\\n",
    "                                verbose=2, mode='min', restore_best_weights=True)\n",
    "        hist_model = model.fit(\n",
    "            [X_train, OC_train, HL_train], y_train, epochs=epochs, batch_size=batch_size, \\\n",
    "            validation_split=0.2, callbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        #得到輸出\n",
    "        y_pred = model.predict([X_test, OC_test, HL_test])\n",
    "        y_pred = y_pred / magnification_slope\n",
    "        columns_names = ['履約價', '隱含波動率({})'.format(s_c)]\n",
    "        column_index = [IV_data.columns.get_loc(col) for col in columns_names]\n",
    "\n",
    "        SSE_everyday = np.zeros(((len(y_pred), K_num)))\n",
    "    \n",
    "        for i in range(len(y_pred)):\n",
    "            K_day = IV_matrix_test[i*K_num:(i+1)*K_num, column_index[0]]\n",
    "            iv_day = IV_matrix_test[i*K_num:(i+1)*K_num, column_index[1]]\n",
    "            y_pred_day = y_pred[i]\n",
    "\n",
    "            ForecastData, SSE_everyday[i] = minSSE_recovery(y=iv_day, x=K_day, slope_yhat=y_pred_day)\n",
    "        sse_everyday = SSE_everyday[:, 0]\n",
    "        allMSE[j] = np.mean(sse_everyday)\n",
    "\n",
    "    loss = np.min(allMSE)\n",
    "    return loss\n",
    "\n",
    "@use_named_args(search_space)\n",
    "def objective(**params):\n",
    "    cnn1_kernel_size1 = params['cnn1_kernel_size1']\n",
    "    cnn1_kernel_size2 = params['cnn1_kernel_size2']\n",
    "    cnn1_filter1 = params['cnn1_filter1']\n",
    "    cnn1_filter2 = params['cnn1_filter2']\n",
    "    cnn2_kernel_size1 = params['cnn2_kernel_size1']\n",
    "    cnn2_kernel_size2 = params['cnn2_kernel_size2']\n",
    "    cnn2_filter1 = params['cnn2_filter1']\n",
    "    cnn2_filter2 = params['cnn2_filter2']\n",
    "    merged_neurons_num1 = params['merged_neurons_num1']\n",
    "    merged_neurons_num2 = params['merged_neurons_num2']\n",
    "    lambda1 = params['lambda1']\n",
    "    lambda2 = params['lambda2']\n",
    "    lambda3 = params['lambda3']\n",
    "    lambda4 = params['lambda4']\n",
    "    \n",
    "    loss = train_and_evaluate_net(cnn1_kernel_size1, cnn1_kernel_size2, cnn1_filter1, cnn1_filter2,\\\n",
    "                           cnn2_kernel_size1, cnn2_kernel_size2, cnn2_filter1, cnn2_filter2,\n",
    "                           merged_neurons_num1, merged_neurons_num2, \\\n",
    "                            lambda1, lambda2, lambda3, lambda4)\n",
    "    return loss\n",
    "\n",
    "\n",
    "result = gp_minimize(objective, search_space, n_calls=n_calls, random_state=42, acq_func='EI')\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"最佳參數: {}\".format(result.x))\n",
    "print(\"最小損失: {}\".format(result.fun))\n",
    "\n",
    "\n",
    "best_params = result.x\n",
    "\n",
    "\n",
    "cnn1_kernel_size1 = best_params[0]\n",
    "cnn1_kernel_size2 = best_params[1]\n",
    "cnn1_filter1 = best_params[2]\n",
    "cnn1_filter2 = best_params[3]\n",
    "cnn2_kernel_size1 = best_params[4]\n",
    "cnn2_kernel_size2 = best_params[5]\n",
    "cnn2_filter1 = best_params[6]\n",
    "cnn2_filter2 = best_params[7]\n",
    "merged_neurons_num1 = best_params[8]\n",
    "merged_neurons_num2 = best_params[9]\n",
    "lambda1 = best_params[10]\n",
    "lambda2 = best_params[11]\n",
    "lambda3 = best_params[12]\n",
    "lambda4 = best_params[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 43ms/step - loss: 5.0380 - mse: 3.8485 - val_loss: 0.8451 - val_mse: 0.6780\n",
      "Epoch 2/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 2.6185 - mse: 2.4888 - val_loss: 0.7283 - val_mse: 0.6594\n",
      "Epoch 3/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - loss: 2.8465 - mse: 2.7852 - val_loss: 0.6345 - val_mse: 0.5880\n",
      "Epoch 4/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 2.4552 - mse: 2.4108 - val_loss: 0.6441 - val_mse: 0.6057\n",
      "Epoch 5/5\n",
      "\u001b[1m56/56\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 2.5933 - mse: 2.5559 - val_loss: 0.6910 - val_mse: 0.6552\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "0.002714392497647732\n"
     ]
    }
   ],
   "source": [
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import json\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, np.int64):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, l2):\n",
    "        return {\"type\": \"L2\", \"l2\": obj.l2}\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    raise TypeError(f\"Object of type {type(obj)} is not JSON serializable\")\n",
    "\n",
    "\n",
    "#決定這次的模型編號\n",
    "model_idx = 1\n",
    "#本次預測的變數為：slope(隱波與履約價之間的斜率)或iv(隱波的數值)\n",
    "forecast_variable = 'slope'\n",
    "#資料設定都相同的情況下，最多儲存多少種模型結果\n",
    "max_model = 20\n",
    "\n",
    "#設定參數\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "Slope_lstm_neurons_num = [int(lstm_neurons_num)]*lstm_layers_num\n",
    "Slope_lstm_activations = ['tanh']*len(Slope_lstm_neurons_num)\n",
    "Slope_lstm_recurrent_activations = ['sigmoid']*len(Slope_lstm_neurons_num)\n",
    "Slope_dense_neurons_num = [dense1_neurons_num]\n",
    "Slope_dense_activations = ['relu']\n",
    "Slope_kernel_regularizer=[l2(lambda1)]*(len(Slope_lstm_neurons_num) + len(Slope_dense_neurons_num))\n",
    "\n",
    "OC_cnn_kernel_size = [(cnn1_kernel_size1, cnn1_kernel_size1),\n",
    "                      (cnn1_kernel_size2, cnn1_kernel_size2)]\n",
    "cnn1_layers_num = len(OC_cnn_kernel_size)\n",
    "OC_cnn_filters = [cnn1_filter1, cnn1_filter2] \n",
    "OC_cnn_strides = [(1,1)] * cnn1_layers_num\n",
    "OC_cnn_padding = ['same'] * cnn1_layers_num \n",
    "OC_cnn_activations = ['relu']*len(OC_cnn_kernel_size)\n",
    "OC_dense_neurons_num = [dense2_neurons_num]\n",
    "OC_dense_activations = ['relu']\n",
    "OC_kernel_regularizer=[l2(lambda2)]*(len(OC_cnn_kernel_size) + len(OC_dense_neurons_num))\n",
    "\n",
    "HL_cnn_kernel_size = [(cnn2_kernel_size1, cnn2_kernel_size1),\n",
    "                      (cnn2_kernel_size2, cnn2_kernel_size2)]\n",
    "cnn2_layers_num = len(HL_cnn_kernel_size)\n",
    "HL_cnn_filters = [cnn2_filter1, cnn2_filter2] \n",
    "HL_cnn_strides = [(1,1)] * cnn2_layers_num\n",
    "HL_cnn_padding = ['same'] * cnn2_layers_num \n",
    "HL_cnn_activations = ['relu']*len(HL_cnn_kernel_size)\n",
    "HL_dense_neurons_num = [dense3_neurons_num]\n",
    "HL_dense_activations = ['relu']\n",
    "HL_kernel_regularizer=[l2(lambda3)]*(len(HL_cnn_kernel_size) + len(HL_dense_neurons_num))\n",
    "\n",
    "    \n",
    "\n",
    "Merged_neurons_num = [merged_neurons_num1, merged_neurons_num2]\n",
    "Megred_activations = ['relu']*len(Merged_neurons_num)\n",
    "Merged_kernel_regularizer=[l2(lambda4)]*(len(Merged_neurons_num))\n",
    "lstm_Slope_structure = {\n",
    "    'input_shape': X_train.shape[1:],\n",
    "    'lstm_neurons_num': Slope_lstm_neurons_num,\n",
    "    'lstm_activations': Slope_lstm_activations,\n",
    "    'lstm_recurrent_activations':Slope_lstm_recurrent_activations,\n",
    "    'dense_neurons_num':Slope_dense_neurons_num,\n",
    "    'dense_activations':Slope_dense_activations,\n",
    "    'kernel_regularizer':Slope_kernel_regularizer\n",
    "    }\n",
    "    \n",
    "OC_cnn_structure = {\n",
    "    'input_shape': OC_train.shape[1:],\n",
    "    'cnn_kernel_size': OC_cnn_kernel_size,\n",
    "    'cnn_filters': OC_cnn_filters,\n",
    "    'cnn_stride': OC_cnn_strides,\n",
    "    'cnn_padding': OC_cnn_padding,\n",
    "    'cnn_activations': OC_cnn_activations,\n",
    "    'dense_neurons_num': OC_dense_neurons_num,\n",
    "    'dense_activations': OC_dense_activations,\n",
    "    'kernel_regularizer': OC_kernel_regularizer\n",
    "    }\n",
    "\n",
    "HL_cnn_structure = {\n",
    "    'input_shape': HL_train.shape[1:],\n",
    "    'cnn_kernel_size': HL_cnn_kernel_size,\n",
    "    'cnn_filters': HL_cnn_filters,\n",
    "    'cnn_stride': HL_cnn_strides,\n",
    "    'cnn_padding': HL_cnn_padding,\n",
    "    'cnn_activations': HL_cnn_activations,\n",
    "    'dense_neurons_num': HL_dense_neurons_num,\n",
    "    'dense_activations': HL_dense_activations,\n",
    "    'kernel_regularizer': HL_kernel_regularizer\n",
    "    }\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "Merged_structure = {\n",
    "        'neurons_num': Merged_neurons_num,\n",
    "        'activations': Megred_activations,\n",
    "        'kernel_regularizer': Merged_kernel_regularizer\n",
    "    }\n",
    "structure_list = [lstm_Slope_structure, OC_cnn_structure, HL_cnn_structure, Merged_structure]\n",
    "structure_name_list = ['lstm_slope_structure', 'cnn_OC_structure','cnn_HL_structure', 'Merged_structure']\n",
    "General_structure = {\n",
    "    structure_name_list[0] : structure_list[0],\n",
    "    structure_name_list[1]: structure_list[1],\n",
    "    structure_name_list[2] : structure_list[2],\n",
    "    structure_name_list[3] : structure_list[3]\n",
    "}\n",
    "\n",
    "\n",
    "def save_model_txt(file_path, structure_list, structure_name_list, mse, mse_adj):\n",
    "    with open('{}.txt'.format(file_path), 'w', encoding='utf-8') as txt_file:\n",
    "        for i in range(len(structure_list)):\n",
    "            txt_file.write(structure_name_list[i] + ':\\n')\n",
    "            for key, value in structure_list[i].items():\n",
    "                txt_file.write(f\"{key}: {value}\\n\")\n",
    "            txt_file.write('-----------------\\n')\n",
    "        txt_file.write('MSE:{}'.format(mse))\n",
    "        txt_file.write('MSE_adj:{}'.format(mse_adj))\n",
    "\n",
    "\n",
    "alltrain = 1\n",
    "allMSE = np.zeros(alltrain)\n",
    "allMSE_adj = np.zeros(alltrain)  \n",
    "allForecastIV = np.zeros((K_num*len(OC_test), alltrain))\n",
    "allSSE_everyday = np.zeros((K_num*len(OC_test), alltrain))\n",
    "allSSE_everyday_adj = np.zeros((K_num*len(OC_test), alltrain))\n",
    "for j in range(alltrain):\n",
    "    #設定模型結構\n",
    "    model = set_CNN_LSTM(lstm_Slope_structure, OC_cnn_structure, HL_cnn_structure, Merged_structure)\n",
    "    model.compile(loss='mse', optimizer = 'adam', metrics=['mse'])  \n",
    "\n",
    "    # 訓練模型\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=10, \\\n",
    "                               verbose=2, mode='min', restore_best_weights=True)\n",
    "    hist_model = model.fit(\n",
    "        [X_train, OC_train, HL_train], y_train, epochs=epochs, batch_size=batch_size, \\\n",
    "        validation_split=0.2, callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #得到輸出\n",
    "    y_pred = model.predict([X_test, OC_test, HL_test])\n",
    "    y_pred = y_pred / magnification_slope\n",
    "    columns_names = ['履約價', '隱含波動率({})'.format(s_c)]\n",
    "    column_index = [IV_data.columns.get_loc(col) for col in columns_names]\n",
    "    ForecastIV = np.zeros(((len(y_pred), K_num)))\n",
    "    SSE_everyday = np.zeros(((len(y_pred), K_num)))\n",
    "    SSE_everyday_adj = np.zeros(((len(y_pred), K_num)))\n",
    "\n",
    "    model_name = 'model{}'.format(model_idx)\n",
    "    #model_file = 'model{}.h5'.format(model_idx)\n",
    "    model_png = 'model{}.png'.format(model_idx)\n",
    "    Forecast_name = 'model{}.csv'.format(model_idx)\n",
    "    model_txt = 'model{}.txt'.format(model_idx)\n",
    "    model_json = 'model{}.json'.format(model_idx)\n",
    "    for i in range(len(y_pred)):\n",
    "        K_day = IV_matrix_test[i*K_num:(i+1)*K_num, column_index[0]]\n",
    "        iv_day = IV_matrix_test[i*K_num:(i+1)*K_num, column_index[1]]\n",
    "        y_pred_day = y_pred[i]\n",
    "        iv_mean = iv_day / np.mean(iv_day)\n",
    "        ForecastIV[i], sse_day = minSSE_recovery(y=iv_day, x=K_day, slope_yhat=y_pred_day)\n",
    "        SSE_everyday[i] = sse_day\n",
    "        SSE_everyday_adj[i] = sse_day / iv_mean\n",
    "\n",
    "    ForecastIV = np.reshape(ForecastIV,(-1,1))\n",
    "    sse_everyday = SSE_everyday[:, 0]\n",
    "    sse_everyday_adj = SSE_everyday_adj[:, 0]\n",
    "    SSE_everyday = np.reshape(SSE_everyday ,(-1,1))\n",
    "    SSE_everyday_adj = np.reshape(SSE_everyday_adj ,(-1,1))\n",
    "    allMSE[j] = np.mean(sse_everyday)\n",
    "    allMSE_adj[j] = np.mean(sse_everyday_adj)\n",
    "    allForecastIV[:,j:j+1] = ForecastIV\n",
    "    allSSE_everyday[:, j:j+1] = SSE_everyday\n",
    "    allSSE_everyday_adj[:, j:j+1] = SSE_everyday_adj\n",
    "\n",
    "min_MSE_idx = np.argsort(allMSE)[0]\n",
    "ForecastIV = allForecastIV[:, min_MSE_idx:min_MSE_idx+1]\n",
    "SSE_everyday = allSSE_everyday[:, min_MSE_idx:min_MSE_idx+1]\n",
    "SSE_everyday_adj = allSSE_everyday_adj[:, min_MSE_idx:min_MSE_idx+1]\n",
    "min_MSE = allMSE[min_MSE_idx]\n",
    "min_MSE_adj = allMSE_adj[min_MSE_idx]\n",
    "\n",
    "Forecast_matrix  = np.hstack((IV_matrix_test, ForecastIV, SSE_everyday, SSE_everyday_adj))\n",
    "column = np.hstack((IV_data.columns.to_numpy(), \\\n",
    "                        np.array(['上市天數(交易日)','預測隱含波動率({})'.format(s_c), 'loss', '調整後loss'])))\n",
    "    \n",
    "   \n",
    "Forecast_Data = pd.DataFrame(data=Forecast_matrix, columns=column)\n",
    "\n",
    "\n",
    "loss_columns_names = ['交易日期', '到期天數', '上市天數(交易日)', 'loss']\n",
    "loss_columns_index = [Forecast_Data.columns.get_loc(col) for col in loss_columns_names]\n",
    "loss_Data = Forecast_Data.iloc[range(0, len(Forecast_Data), K_num), loss_columns_index]\n",
    "loss_Data = loss_Data.reset_index().iloc[:,1:]\n",
    "loss_Data = loss_Data.rename(columns={'loss': model_name})\n",
    "MSE_data = pd.DataFrame(columns=loss_Data.columns, data=[['MSE', 'MSE', 'MSE', min_MSE]])\n",
    "loss_Data = pd.concat([loss_Data, MSE_data], axis=0)\n",
    "loss_Data = loss_Data.reset_index().iloc[:,1:]\n",
    "print(min_MSE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_path = top_path\n",
    "model_type = 'CNN-LSTM'\n",
    "\n",
    "model_Dir_tree = ['Forecast&model', expiry, IV_type, 'K_{}'.format(K_Range_dir), model_type,\\\n",
    "                  'seq{}_seq{}_min{}'.format(seq_length1, seq_length2, min_contract_day), forecast_variable]\n",
    "for model_dir in model_Dir_tree:\n",
    "    if model_dir not in os.listdir(model_path):\n",
    "        os.mkdir(model_path + model_dir)\n",
    "    model_path = model_path + model_dir + '/'\n",
    "\n",
    "loss_adj_columns_names = ['交易日期', '到期天數', '調整後loss']\n",
    "loss_adj_columns_index = [Forecast_Data.columns.get_loc(col) for col in loss_adj_columns_names]\n",
    "loss_adj_Data = Forecast_Data.iloc[range(0, len(Forecast_Data), K_num), loss_adj_columns_index]\n",
    "loss_adj_Data = loss_adj_Data.reset_index().iloc[:,1:]\n",
    "loss_adj_Data = loss_adj_Data.rename(columns={'調整後loss': model_name})\n",
    "MSE_adj_data = pd.DataFrame(columns=loss_adj_Data.columns, data=[['MSE', 'MSE', min_MSE_adj]])\n",
    "loss_adj_Data = pd.concat([loss_adj_Data, MSE_adj_data], axis=0)\n",
    "loss_adj_Data = loss_adj_Data.reset_index().iloc[:,1:]\n",
    "\n",
    "\n",
    "\n",
    "if 'Modelloss.csv' in os.listdir(model_path):\n",
    "    Modelloss = pd.read_csv(model_path + 'Modelloss.csv', index_col=False, encoding='Big5')\n",
    "    Modelloss_adj = pd.read_csv(model_path + 'Modelloss_adj.csv', index_col=False, encoding='Big5')\n",
    "    if model_name in Modelloss.columns:\n",
    "        if Modelloss[model_name][len(Modelloss)-1] > min_MSE:\n",
    "            Modelloss[model_name] = loss_Data[model_name]\n",
    "            Modelloss_adj[model_name] = loss_adj_Data[model_name]\n",
    "            Forecast_Data.to_csv(model_path + Forecast_name, index=False, encoding='Big5')\n",
    "            #model.save(model_path + model_file)\n",
    "            with open(model_path+model_json, 'w', encoding='utf-8') as json_file:\n",
    "                    json.dump(General_structure, json_file, ensure_ascii=False, indent=4, \\\n",
    "                              default=convert_to_serializable)\n",
    "            save_model_txt(model_path+model_txt, structure_list, structure_name_list, min_MSE, min_MSE_adj)\n",
    "            plot_model(model, to_file=model_path + model_png, show_shapes=True, show_layer_names=False)\n",
    "            print('原本的{}已經被替換'.format(model_name))\n",
    "        else:\n",
    "            print('此模型未被儲存')\n",
    "    elif len(Modelloss.columns) >= max_model+2 :\n",
    "        all_MSE = np.array(Modelloss.iloc[-1, 2:])\n",
    "        max_MSE = np.max(all_MSE)\n",
    "        if max_MSE > min_MSE:\n",
    "            max_MSE_idx = (np.arange(len(all_MSE))[np.equal(all_MSE, max_MSE)])[0] +2\n",
    "            Modelloss.iloc[:,max_MSE_idx] = loss_Data[model_name]\n",
    "            Modelloss_adj.iloc[:,max_MSE_idx] = loss_adj_Data[model_name]\n",
    "            model_name = Modelloss.columns[max_MSE_idx]\n",
    "            Forecast_name = model_name + '.csv'\n",
    "            #model_file = model_name + '.h5'\n",
    "            model_png = model_name + '.png'\n",
    "            Forecast_Data.to_csv(model_path + Forecast_name, index=False, encoding='Big5')\n",
    "            #model.save(model_path + model_file)\n",
    "            with open(model_path+model_json, 'w', encoding='utf-8') as json_file:\n",
    "                    json.dump(General_structure, json_file, ensure_ascii=False, indent=4, \\\n",
    "                              default=convert_to_serializable)\n",
    "            save_model_txt(model_path+model_txt, structure_list, structure_name_list, min_MSE, min_MSE_adj)\n",
    "            plot_model(model, to_file=model_path + model_png, show_shapes=True, show_layer_names=False)\n",
    "            print('原本的{}已經被替換'.format(model_name))\n",
    "        else:\n",
    "            print('此模型未被儲存')\n",
    "    else:\n",
    "        Modelloss = pd.concat([Modelloss, loss_Data[model_name]], axis=1)\n",
    "        Modelloss = Modelloss.reset_index().iloc[:,1:] \n",
    "        Modelloss_adj = pd.concat([Modelloss_adj, loss_Data[model_name]], axis=1)\n",
    "        Modelloss_adj = Modelloss_adj.reset_index().iloc[:,1:] \n",
    "        Forecast_Data.to_csv(model_path + Forecast_name, index=False, encoding='Big5')\n",
    "        #model.save(model_path + model_file)\n",
    "        with open(model_path+model_json, 'w', encoding='utf-8') as json_file:\n",
    "                    json.dump(General_structure, json_file, ensure_ascii=False, indent=4, \\\n",
    "                              default=convert_to_serializable)\n",
    "        save_model_txt(model_path+model_txt, structure_list, structure_name_list, min_MSE, min_MSE_adj)\n",
    "        plot_model(model, to_file=model_path + model_png, show_shapes=True, show_layer_names=False)\n",
    "        print('此模型已經被儲存為{}'.format(model_name))\n",
    "else:\n",
    "    Modelloss = loss_Data\n",
    "    Modelloss_adj = loss_adj_Data\n",
    "    Forecast_Data.to_csv(model_path + Forecast_name, index=False, encoding='Big5')\n",
    "    #model.save(model_path + model_file)\n",
    "    with open(model_path+model_json, 'w', encoding='utf-8') as json_file:\n",
    "                    json.dump(General_structure, json_file, ensure_ascii=False, indent=4, \\\n",
    "                              default=convert_to_serializable)\n",
    "    save_model_txt(model_path+model_txt, structure_list, structure_name_list, min_MSE, min_MSE_adj)\n",
    "    plot_model(model, to_file=model_path + model_png, show_shapes=True, show_layer_names=False)\n",
    "\n",
    "    \n",
    "Modelloss.to_csv(model_path + 'Modelloss.csv', index=False, encoding='Big5')\n",
    "Modelloss_adj.to_csv(model_path + 'Modelloss_adj.csv', index=False, encoding='Big5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
